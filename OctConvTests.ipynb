{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octave Convolution Tests\n",
    "\n",
    "We can use this notebook to test our implementation of the OctConv module.\n",
    "\n",
    "The OctConv module itself is defined under `modules.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from modules import OctConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing OctConv Behavior\n",
    "\n",
    "Maybe we can move this to a `test.py` file eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output shapes for 1x1 convolution\n",
    "\n",
    "oc = OctConv2d(16, 16, (1, 1), 0.5, 0.5)\n",
    "input_h = torch.randn(128, 8, 32, 32)\n",
    "input_l = torch.randn(128, 8, 16, 16)\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "assert output_h.shape == (128, 8, 32, 32), \"Incorrect high-frequency output shape for OctConv2d\"\n",
    "assert output_l.shape == (128, 8, 16, 16), \"Incorrect low-frequency output shape for OctConv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output shapes with alpha_in != alpha_out\n",
    "\n",
    "oc = OctConv2d(16, 16, (1, 1), 0.5, 0.25)\n",
    "input_h = torch.randn(128, 8, 32, 32)\n",
    "input_l = torch.randn(128, 8, 16, 16)\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "assert output_h.shape == (128, 12, 32, 32), \"Incorrect high-frequency output shape for OctConv2d\"\n",
    "assert output_l.shape == (128, 4, 16, 16), \"Incorrect low-frequency output shape for OctConv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output shapes with alpha_in != alpha_out and in_channels != out_channels\n",
    "\n",
    "oc = OctConv2d(16, 32, (1, 1), 0.5, 0.25)\n",
    "input_h = torch.randn(128, 8, 32, 32)\n",
    "input_l = torch.randn(128, 8, 16, 16)\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "assert output_h.shape == (128, 24, 32, 32), \"Incorrect high-frequency output shape for OctConv2d\"\n",
    "assert output_l.shape == (128, 8, 16, 16), \"Incorrect low-frequency output shape for OctConv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output shapes with alpha_in = alpha_out = 0\n",
    "\n",
    "oc = OctConv2d(16, 32, (1, 1), 0, 0)\n",
    "input_h = torch.randn(128, 16, 32, 32)\n",
    "input_l = torch.randn(128, 0, 16, 16)\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "assert output_h.shape == (128, 32, 32, 32), \"Incorrect high-frequency output shape for OctConv2d\"\n",
    "assert output_l is None, \"Incorrect low-frequency output shape for OctConv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output shapes with alpha_in = 0, alpha_out > 0 (imitates first layer)\n",
    "\n",
    "oc = OctConv2d(16, 32, (1, 1), 0, 0.25)\n",
    "input_h = torch.randn(128, 16, 32, 32)\n",
    "input_l = None\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "assert output_h.shape == (128, 24, 32, 32), \"Incorrect high-frequency output shape for OctConv2d\"\n",
    "assert output_l.shape == (128, 8, 16, 16), \"Incorrect low-frequency output shape for OctConv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output shapes with padding and stride\n",
    "\n",
    "oc = OctConv2d(16, 32, (3, 3), 0.5, 0.5, stride=1, padding=1)\n",
    "input_h = torch.randn(128, 8, 32, 32)\n",
    "input_l = torch.randn(128, 8, 16, 16)\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "assert output_h.shape == (128, 16, 32, 32), \"Shape mismatch for stride=1, padding=1\"\n",
    "assert output_l.shape == (128, 16, 16, 16), \"Shape mismatch for stride=1, padding=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test output shapes with stride to downsample\n",
    "\n",
    "oc = OctConv2d(16, 32, (2, 2), 0.5, 0.5, stride=2, padding=0)\n",
    "input_h = torch.randn(128, 8, 32, 32)\n",
    "input_l = torch.randn(128, 8, 16, 16)\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "assert output_h.shape == (128, 16, 16, 16), \"Shape mismatch for stride=2, padding=0\"\n",
    "assert output_l.shape == (128, 16, 8, 8), \"Shape mismatch for stride=2, padding=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that OctConv2d behaves like Conv2d when alpha_in = alpha_out = 0\n",
    "\n",
    "oc = OctConv2d(3, 32, (3, 3), 0, 0, stride=1, padding=1)\n",
    "conv = nn.Conv2d(3, 32, (3, 3), stride=1, padding=1)\n",
    "input_h = torch.randn(128, 3, 32, 32)\n",
    "input_l = None\n",
    "\n",
    "conv.weight = oc.conv_hh.weight\n",
    "conv.bias = oc.conv_hh.bias\n",
    "\n",
    "output_h, output_l = oc(input_h, input_l)\n",
    "output_conv = conv(input_h)\n",
    "assert output_h.shape == output_conv.shape, \"OctConv2d and Conv2d have different output shapes\"\n",
    "assert torch.all(torch.eq(output_h, output_conv)), \"OctConv2d and Conv2d have different outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Octconv Network\n",
    "\n",
    "Lol right now this code is really specific and not very flexible - we can write code to initialize more general Octconv networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a flatten method here for convenience (taken from Pytorch notebook assignment 2)\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use nn.sequential because sequential only takes one input at each stage\n",
    "# We have two outputs/inputs at each layer, for the low-frequency and high-frequency channels\n",
    "\n",
    "# We could store the layers more neatly using ModuleDict or ModuleList\n",
    "\n",
    "class FourLayerOctConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Four layer octconv net for testing. Assumes inputs are of size 3 x 32 x 32.\n",
    "    \n",
    "    Architecture: [Octconv -> ReLU -> OctConv -> ReLU -> Pool]*2 -> FC\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, F, D_out):\n",
    "        \"\"\"\n",
    "        Initialize a four-layer Octconv network.\n",
    "        \n",
    "        alpha: float between 0 and 1 representing our alpha parameter\n",
    "        F: integer representing the number of filters in each hidden layer\n",
    "        D_out: the length of the output vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.oc1 = OctConv2d(3, F, (3, 3), 0, self.alpha, stride=1, padding=1)\n",
    "        self.oc2 = OctConv2d(F, F, (3, 3), self.alpha, self.alpha, stride=1, padding=1)\n",
    "        self.oc3 = OctConv2d(F, F, (3, 3), self.alpha, self.alpha, stride=1, padding=1)\n",
    "        self.oc4 = OctConv2d(F, F, (3, 3), self.alpha, 0, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(F * 8 * 8, D_out)\n",
    "        # TODO: Do we need to initialize the weights of oc layers under the OctConv module?\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_h, x_l = self.oc1(x, None) # alpha_in = 0\n",
    "        x_h, x_l = F.relu(x_h), F.relu(x_l)\n",
    "        x_h, x_l = self.oc2(x_h, x_l)\n",
    "        x_h, x_l = F.relu(x_h), F.relu(x_l)\n",
    "        x_h, x_l = F.max_pool2d(x_h, (2, 2), stride=2), F.max_pool2d(x_l, (2, 2), stride=2)\n",
    "        \n",
    "        x_h, x_l = self.oc3(x_h, x_l)\n",
    "        x_h, x_l = F.relu(x_h), F.relu(x_l)\n",
    "        x_h, _ = self.oc4(x_h, x_l) # alpha_out = 0\n",
    "        x_h = F.relu(x_h)\n",
    "        x_h = F.max_pool2d(x_h, (2, 2), stride=2)\n",
    "        x_h = flatten(x_h)\n",
    "        \n",
    "        out = self.fc1(x_h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random training data\n",
    "N, C, H, W, D_out = 10, 3, 32, 32, 10\n",
    "x = torch.randn(N, C, H, W, dtype=dtype, device=device)\n",
    "y = torch.randint(0, D_out, (D_out, ), dtype=dtype, device=device) # Random correct indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FourLayerOctConvNet(\n",
       "   (oc1): OctConv2d(\n",
       "     (conv_hh): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv_hl): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (pool): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
       "   )\n",
       "   (oc2): OctConv2d(\n",
       "     (conv_hh): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv_ll): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv_lh): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (upsample): Upsample(scale_factor=2, mode=nearest)\n",
       "     (conv_hl): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (pool): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
       "   )\n",
       "   (oc3): OctConv2d(\n",
       "     (conv_hh): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv_ll): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv_lh): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (upsample): Upsample(scale_factor=2, mode=nearest)\n",
       "     (conv_hl): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (pool): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
       "   )\n",
       "   (oc4): OctConv2d(\n",
       "     (conv_hh): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv_lh): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (upsample): Upsample(scale_factor=2, mode=nearest)\n",
       "   )\n",
       "   (fc1): Linear(in_features=2048, out_features=10, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our model\n",
    "model = FourLayerOctConvNet(0.25, 32, 10)\n",
    "list(model.modules())[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0476,  0.0253,  0.0508, -0.0035,  0.1010,  0.0477, -0.0843, -0.0289,\n",
       "         -0.0328,  0.0324]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try one forward pass\n",
    "y_pred = model(x)\n",
    "y_pred[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03386716917157173\n",
      "1 0.030994605273008347\n",
      "2 0.028363799676299095\n",
      "3 0.02621612511575222\n",
      "4 0.024231623858213425\n",
      "5 0.022223185747861862\n",
      "6 0.020702075213193893\n",
      "7 0.019034957513213158\n",
      "8 0.017602253705263138\n",
      "9 0.016336917877197266\n",
      "10 0.015036201104521751\n",
      "11 0.01393346767872572\n",
      "12 0.01289587002247572\n",
      "13 0.011908340267837048\n",
      "14 0.011047649197280407\n",
      "15 0.01022186316549778\n",
      "16 0.009447669610381126\n",
      "17 0.008777523413300514\n",
      "18 0.008153152652084827\n",
      "19 0.007551670074462891\n",
      "20 0.00701484689489007\n",
      "21 0.0065367696806788445\n",
      "22 0.006086921785026789\n",
      "23 0.005668449215590954\n",
      "24 0.005291747860610485\n",
      "25 0.0049461363814771175\n",
      "26 0.0046257018111646175\n",
      "27 0.0043357848189771175\n",
      "28 0.004071235656738281\n",
      "29 0.0038236617110669613\n",
      "30 0.0035923004616051912\n",
      "31 0.00338325509801507\n",
      "32 0.00319499964825809\n",
      "33 0.0030204772483557463\n",
      "34 0.002855491591617465\n",
      "35 0.00270423898473382\n",
      "36 0.00256519322283566\n",
      "37 0.00243721017614007\n",
      "38 0.0023181915748864412\n",
      "39 0.00220661167986691\n",
      "40 0.0021038055419921875\n",
      "41 0.0020080567337572575\n",
      "42 0.001918792724609375\n",
      "43 0.0018352508777752519\n",
      "44 0.0017572402721270919\n",
      "45 0.0016853332053869963\n",
      "46 0.0016178131336346269\n",
      "47 0.001554298447445035\n",
      "48 0.0014949798351153731\n",
      "49 0.0014385223621502519\n",
      "50 0.0013866424560546875\n",
      "51 0.001337242079898715\n",
      "52 0.00129108433611691\n",
      "53 0.0012470245128497481\n",
      "54 0.0012054443359375\n",
      "55 0.0011663436889648438\n",
      "56 0.0011299133766442537\n",
      "57 0.0010946274269372225\n",
      "58 0.0010608673328533769\n",
      "59 0.0010290145874023438\n",
      "60 0.0009990691905841231\n",
      "61 0.0009706497075967491\n",
      "62 0.0009433746454305947\n",
      "63 0.0009174346923828125\n",
      "64 0.00089263916015625\n",
      "65 0.0008686065557412803\n",
      "66 0.0008462906116619706\n",
      "67 0.0008251190301962197\n",
      "68 0.0008045196300372481\n",
      "69 0.0007844924693927169\n",
      "70 0.0007654189830645919\n",
      "71 0.0007471084827557206\n",
      "72 0.0007297515985555947\n",
      "73 0.0007125854608602822\n",
      "74 0.0006963729974813759\n",
      "75 0.0006811142084188759\n",
      "76 0.0006656646728515625\n",
      "77 0.0006515503046102822\n",
      "78 0.0006374359363690019\n",
      "79 0.0006240844959393144\n",
      "80 0.0006109237438067794\n",
      "81 0.0005979537963867188\n",
      "82 0.0005861282115802169\n",
      "83 0.0005739211919717491\n",
      "84 0.0005628585931845009\n",
      "85 0.0005516052478924394\n",
      "86 0.0005409240839071572\n",
      "87 0.0005308151012286544\n",
      "88 0.0005203246837481856\n",
      "89 0.0005105972522869706\n",
      "90 0.0005012511974200606\n",
      "91 0.0004919052007608116\n",
      "92 0.0004833221319131553\n",
      "93 0.00047435759915970266\n",
      "94 0.0004661559942178428\n",
      "95 0.00045757292537018657\n",
      "96 0.00044956206693314016\n",
      "97 0.0004417419550009072\n",
      "98 0.00043430327787064016\n",
      "99 0.0004268646298442036\n",
      "100 0.0004199981631245464\n",
      "101 0.0004127502324990928\n",
      "102 0.0004062652587890625\n",
      "103 0.00039920807466842234\n",
      "104 0.000392913818359375\n",
      "105 0.00038661956205032766\n",
      "106 0.0003807067987509072\n",
      "107 0.0003746032598428428\n",
      "108 0.0003688812139444053\n",
      "109 0.00036296845064498484\n",
      "110 0.0003574371221475303\n",
      "111 0.0003517150762490928\n",
      "112 0.00034656524076126516\n",
      "113 0.0003414154052734375\n",
      "114 0.0003364562871865928\n",
      "115 0.0003314971982035786\n",
      "116 0.0003265380801167339\n",
      "117 0.00032176970853470266\n",
      "118 0.00031719208345748484\n",
      "119 0.0003124237118754536\n",
      "120 0.0003078460576944053\n",
      "121 0.0003040313604287803\n",
      "122 0.00029964448185637593\n",
      "123 0.0002952575741801411\n",
      "124 0.00029087066650390625\n",
      "125 0.0002872467157430947\n",
      "126 0.0002834320184774697\n",
      "127 0.0002792358282022178\n",
      "128 0.0002754211309365928\n",
      "129 0.0002719879266805947\n",
      "130 0.00026836394681595266\n",
      "131 0.00026454924955032766\n",
      "132 0.00026111601619049907\n",
      "133 0.00025768281193450093\n",
      "134 0.00025424957857467234\n",
      "135 0.0002513885556254536\n",
      "136 0.000247955322265625\n",
      "137 0.0002449035528115928\n",
      "138 0.0002418518124613911\n",
      "139 0.0002386093110544607\n",
      "140 0.00023574828810524195\n",
      "141 0.000232696533203125\n",
      "142 0.00023021697415970266\n",
      "143 0.0002273559512104839\n",
      "144 0.00022449492826126516\n",
      "145 0.0002216339053120464\n",
      "146 0.00021877288236282766\n",
      "147 0.00021667480177711695\n",
      "148 0.000213623046875\n",
      "149 0.0002113342343363911\n",
      "150 0.00020885467529296875\n",
      "151 0.0002061843842966482\n",
      "152 0.00020351409330032766\n",
      "153 0.00020141601271461695\n",
      "154 0.00019931793212890625\n",
      "155 0.00019702911959029734\n",
      "156 0.000194549560546875\n",
      "157 0.0001926422119140625\n",
      "158 0.00019016265287064016\n",
      "159 0.00018825530423782766\n",
      "160 0.00018615722365211695\n",
      "161 0.00018405914306640625\n",
      "162 0.00018177033052779734\n",
      "163 0.00017986298189498484\n",
      "164 0.00017795563326217234\n",
      "165 0.00017623901658225805\n",
      "166 0.0001739501894917339\n",
      "167 0.000171661376953125\n",
      "168 0.00017051697068382055\n",
      "169 0.00016841889009810984\n",
      "170 0.0001663207949604839\n",
      "171 0.00016479492478538305\n",
      "172 0.00016345977201126516\n",
      "173 0.00016136169142555445\n",
      "174 0.00015964507474564016\n",
      "175 0.0001581192045705393\n",
      "176 0.0001567840517964214\n",
      "177 0.00015506744966842234\n",
      "178 0.00015316010103560984\n",
      "179 0.00015182494826149195\n",
      "180 0.00014991759962867945\n",
      "181 0.0001485824614064768\n",
      "182 0.0001470565766794607\n",
      "183 0.00014553070650435984\n",
      "184 0.00014419555373024195\n",
      "185 0.00014247893705032766\n",
      "186 0.0001413345307810232\n",
      "187 0.00014019012451171875\n",
      "188 0.0001388549862895161\n",
      "189 0.0001371383696096018\n",
      "190 0.0001358032168354839\n",
      "191 0.00013446807861328125\n",
      "192 0.000133514404296875\n",
      "193 0.00013236999802757055\n",
      "194 0.00013084411330055445\n",
      "195 0.00012969970703125\n",
      "196 0.0001279830903513357\n",
      "197 0.00012702941603492945\n",
      "198 0.0001256942778127268\n",
      "199 0.00012474060349632055\n",
      "200 0.0001237869291799143\n",
      "201 0.00012264252291060984\n",
      "202 0.00012130737013649195\n",
      "203 0.0001203536958200857\n",
      "204 0.00011940002150367945\n",
      "205 0.000118255615234375\n",
      "206 0.00011749267287086695\n",
      "207 0.00011596679541980848\n",
      "208 0.00011501312110340223\n",
      "209 0.00011367797560524195\n",
      "210 0.00011310577247058973\n",
      "211 0.00011196136620128527\n",
      "212 0.00011138916306663305\n",
      "213 0.00010986327833961695\n",
      "214 0.00010929107520496473\n",
      "215 0.00010833740088855848\n",
      "216 0.00010719299461925402\n",
      "217 0.00010643005225574598\n",
      "218 0.00010566711716819555\n",
      "219 0.00010452270362293348\n",
      "220 0.00010395050048828125\n",
      "221 0.0001028060942189768\n",
      "222 0.00010223388380836695\n",
      "223 0.0001012802094919607\n",
      "224 0.00010013580322265625\n",
      "225 9.956360008800402e-05\n",
      "226 9.841918654274195e-05\n",
      "227 9.784698340808973e-05\n",
      "228 9.72747802734375e-05\n",
      "229 9.613037400413305e-05\n",
      "230 9.55581635935232e-05\n",
      "231 9.460448927711695e-05\n",
      "232 9.403228614246473e-05\n",
      "233 9.307861182605848e-05\n",
      "234 9.250640869140625e-05\n",
      "235 9.1552734375e-05\n",
      "236 9.098053124034777e-05\n",
      "237 9.040832810569555e-05\n",
      "238 8.96453857421875e-05\n",
      "239 8.907318260753527e-05\n",
      "240 8.811950829112902e-05\n",
      "241 8.75473051564768e-05\n",
      "242 8.659363084007055e-05\n",
      "243 8.621215965831652e-05\n",
      "244 8.525848534191027e-05\n",
      "245 8.487701416015625e-05\n",
      "246 8.41140717966482e-05\n",
      "247 8.354186866199598e-05\n",
      "248 8.296966552734375e-05\n",
      "249 8.239746239269152e-05\n",
      "250 8.20159912109375e-05\n",
      "251 8.087158494163305e-05\n",
      "252 8.02993745310232e-05\n",
      "253 7.972717139637098e-05\n",
      "254 7.934570021461695e-05\n",
      "255 7.858276512706652e-05\n",
      "256 7.83920258982107e-05\n",
      "257 7.743835158180445e-05\n",
      "258 7.705688767600805e-05\n",
      "259 7.667541649425402e-05\n",
      "260 7.61032133596018e-05\n",
      "261 7.534027099609375e-05\n",
      "262 7.476806786144152e-05\n",
      "263 7.43865966796875e-05\n",
      "264 7.362365431617945e-05\n",
      "265 7.286071922862902e-05\n",
      "266 7.26699799997732e-05\n",
      "267 7.209777686512098e-05\n",
      "268 7.190704491222277e-05\n",
      "269 7.114410254871473e-05\n",
      "270 7.03811674611643e-05\n",
      "271 6.999969627941027e-05\n",
      "272 6.980895705055445e-05\n",
      "273 6.942749314475805e-05\n",
      "274 6.866455078125e-05\n",
      "275 6.809234764659777e-05\n",
      "276 6.771087646484375e-05\n",
      "277 6.732940528308973e-05\n",
      "278 6.67572021484375e-05\n",
      "279 6.637573096668348e-05\n",
      "280 6.599425978492945e-05\n",
      "281 6.542205665027723e-05\n",
      "282 6.50405854685232e-05\n",
      "283 6.446838233387098e-05\n",
      "284 6.408691115211695e-05\n",
      "285 6.31332368357107e-05\n",
      "286 6.29425048828125e-05\n",
      "287 6.29425048828125e-05\n",
      "288 6.217956251930445e-05\n",
      "289 6.179809861350805e-05\n",
      "290 6.12258882028982e-05\n",
      "291 6.084442065912299e-05\n",
      "292 6.027221752447076e-05\n",
      "293 6.008148193359375e-05\n",
      "294 6.008148193359375e-05\n",
      "295 5.950927879894152e-05\n",
      "296 5.855560448253527e-05\n",
      "297 5.836486889165826e-05\n",
      "298 5.817413330078125e-05\n",
      "299 5.798339770990424e-05\n",
      "300 5.760193016612902e-05\n",
      "301 5.7220458984375e-05\n",
      "302 5.645752025884576e-05\n",
      "303 5.626678466796875e-05\n",
      "304 5.626678466796875e-05\n",
      "305 5.588531348621473e-05\n",
      "306 5.53131103515625e-05\n",
      "307 5.512237476068549e-05\n",
      "308 5.455017162603326e-05\n",
      "309 5.435943603515625e-05\n",
      "310 5.397796485340223e-05\n",
      "311 5.397796485340223e-05\n",
      "312 5.321502612787299e-05\n",
      "313 5.321502612787299e-05\n",
      "314 5.283355858409777e-05\n",
      "315 5.245208740234375e-05\n",
      "316 5.207061622058973e-05\n",
      "317 5.207061622058973e-05\n",
      "318 5.168914867681451e-05\n",
      "319 5.111694190418348e-05\n",
      "320 5.092620995128527e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 5.073547436040826e-05\n",
      "322 5.035400317865424e-05\n",
      "323 4.978180004400201e-05\n",
      "324 4.9591064453125e-05\n",
      "325 4.882812572759576e-05\n",
      "326 4.863739013671875e-05\n",
      "327 4.863739013671875e-05\n",
      "328 4.844665454584174e-05\n",
      "329 4.825591895496473e-05\n",
      "330 4.787445141118951e-05\n",
      "331 4.76837158203125e-05\n",
      "332 4.730224463855848e-05\n",
      "333 4.653930591302924e-05\n",
      "334 4.653930591302924e-05\n",
      "335 4.653930591302924e-05\n",
      "336 4.615783836925402e-05\n",
      "337 4.596710277837701e-05\n",
      "338 4.539489600574598e-05\n",
      "339 4.539489600574598e-05\n",
      "340 4.520416405284777e-05\n",
      "341 4.520416405284777e-05\n",
      "342 4.444122168933973e-05\n",
      "343 4.405975414556451e-05\n",
      "344 4.38690185546875e-05\n",
      "345 4.38690185546875e-05\n",
      "346 4.367828296381049e-05\n",
      "347 4.329681542003527e-05\n",
      "348 4.291534423828125e-05\n",
      "349 4.291534423828125e-05\n",
      "350 4.272460864740424e-05\n",
      "351 4.253387305652723e-05\n",
      "352 4.215240551275201e-05\n",
      "353 4.1961669921875e-05\n",
      "354 4.1961669921875e-05\n",
      "355 4.158019874012098e-05\n",
      "356 4.119873119634576e-05\n",
      "357 4.100799560546875e-05\n",
      "358 4.100799560546875e-05\n",
      "359 4.062652442371473e-05\n",
      "360 4.062652442371473e-05\n",
      "361 4.043579247081652e-05\n",
      "362 4.024505687993951e-05\n",
      "363 3.986358569818549e-05\n",
      "364 3.986358569818549e-05\n",
      "365 3.948211815441027e-05\n",
      "366 3.910064697265625e-05\n",
      "367 3.890991138177924e-05\n",
      "368 3.890991138177924e-05\n",
      "369 3.890991138177924e-05\n",
      "370 3.871917579090223e-05\n",
      "371 3.833770824712701e-05\n",
      "372 3.814697265625e-05\n",
      "373 3.776550147449598e-05\n",
      "374 3.776550147449598e-05\n",
      "375 3.757476952159777e-05\n",
      "376 3.700256274896674e-05\n",
      "377 3.700256274896674e-05\n",
      "378 3.681182715808973e-05\n",
      "379 3.643035961431451e-05\n",
      "380 3.643035961431451e-05\n",
      "381 3.643035961431451e-05\n",
      "382 3.643035961431451e-05\n",
      "383 3.585815284168348e-05\n",
      "384 3.566742088878527e-05\n",
      "385 3.566742088878527e-05\n",
      "386 3.547668529790826e-05\n",
      "387 3.528594970703125e-05\n",
      "388 3.528594970703125e-05\n",
      "389 3.471374657237902e-05\n",
      "390 3.471374657237902e-05\n",
      "391 3.452301098150201e-05\n",
      "392 3.4332275390625e-05\n",
      "393 3.376007225597277e-05\n",
      "394 3.376007225597277e-05\n",
      "395 3.395080420887098e-05\n",
      "396 3.318786548334174e-05\n",
      "397 3.318786548334174e-05\n",
      "398 3.318786548334174e-05\n",
      "399 3.318786548334174e-05\n",
      "400 3.356933666509576e-05\n",
      "401 3.261566234868951e-05\n",
      "402 3.261566234868951e-05\n",
      "403 3.223419116693549e-05\n",
      "404 3.204345557605848e-05\n",
      "405 3.204345557605848e-05\n",
      "406 3.185272362316027e-05\n",
      "407 3.166198803228326e-05\n",
      "408 3.147125244140625e-05\n",
      "409 3.108978125965223e-05\n",
      "410 3.089904930675402e-05\n",
      "411 3.089904930675402e-05\n",
      "412 3.089904930675402e-05\n",
      "413 3.089904930675402e-05\n",
      "414 3.089904930675402e-05\n",
      "415 3.070831371587701e-05\n",
      "416 3.0517578125e-05\n",
      "417 3.0517578125e-05\n",
      "418 3.013610876223538e-05\n",
      "419 2.994537317135837e-05\n",
      "420 2.994537317135837e-05\n",
      "421 2.994537317135837e-05\n",
      "422 2.956390380859375e-05\n",
      "423 2.956390380859375e-05\n",
      "424 2.937316821771674e-05\n",
      "425 2.937316821771674e-05\n",
      "426 2.918243444582913e-05\n",
      "427 2.880096508306451e-05\n",
      "428 2.880096508306451e-05\n",
      "429 2.86102294921875e-05\n",
      "430 2.841949390131049e-05\n",
      "431 2.822876012942288e-05\n",
      "432 2.803802453854587e-05\n",
      "433 2.784729076665826e-05\n",
      "434 2.784729076665826e-05\n",
      "435 2.784729076665826e-05\n",
      "436 2.784729076665826e-05\n",
      "437 2.784729076665826e-05\n",
      "438 2.784729076665826e-05\n",
      "439 2.765655517578125e-05\n",
      "440 2.765655517578125e-05\n",
      "441 2.746581958490424e-05\n",
      "442 2.727508581301663e-05\n",
      "443 2.727508581301663e-05\n",
      "444 2.727508581301663e-05\n",
      "445 2.708435022213962e-05\n",
      "446 2.689361645025201e-05\n",
      "447 2.6702880859375e-05\n",
      "448 2.632141149661038e-05\n",
      "449 2.593994213384576e-05\n",
      "450 2.593994213384576e-05\n",
      "451 2.593994213384576e-05\n",
      "452 2.574920654296875e-05\n",
      "453 2.574920654296875e-05\n",
      "454 2.574920654296875e-05\n",
      "455 2.555847095209174e-05\n",
      "456 2.555847095209174e-05\n",
      "457 2.555847095209174e-05\n",
      "458 2.555847095209174e-05\n",
      "459 2.517700158932712e-05\n",
      "460 2.498626781743951e-05\n",
      "461 2.498626781743951e-05\n",
      "462 2.498626781743951e-05\n",
      "463 2.47955322265625e-05\n",
      "464 2.47955322265625e-05\n",
      "465 2.460479663568549e-05\n",
      "466 2.460479663568549e-05\n",
      "467 2.460479663568549e-05\n",
      "468 2.441406286379788e-05\n",
      "469 2.403259350103326e-05\n",
      "470 2.384185791015625e-05\n",
      "471 2.384185791015625e-05\n",
      "472 2.384185791015625e-05\n",
      "473 2.365112231927924e-05\n",
      "474 2.365112231927924e-05\n",
      "475 2.365112231927924e-05\n",
      "476 2.288818359375e-05\n",
      "477 2.288818359375e-05\n",
      "478 2.288818359375e-05\n",
      "479 2.288818359375e-05\n",
      "480 2.288818359375e-05\n",
      "481 2.326965295651462e-05\n",
      "482 2.326965295651462e-05\n",
      "483 2.307891918462701e-05\n",
      "484 2.307891918462701e-05\n",
      "485 2.269744800287299e-05\n",
      "486 2.269744800287299e-05\n",
      "487 2.269744800287299e-05\n",
      "488 2.269744800287299e-05\n",
      "489 2.250671423098538e-05\n",
      "490 2.212524486822076e-05\n",
      "491 2.174377368646674e-05\n",
      "492 2.174377368646674e-05\n",
      "493 2.155303991457913e-05\n",
      "494 2.155303991457913e-05\n",
      "495 2.155303991457913e-05\n",
      "496 2.136230432370212e-05\n",
      "497 2.174377368646674e-05\n",
      "498 2.174377368646674e-05\n",
      "499 2.136230432370212e-05\n"
     ]
    }
   ],
   "source": [
    "# Overfit on our fake dataset\n",
    "# This training code shamelessy adapted from Justin Johnson's Pytorch examples\n",
    "model = model.to(device=device)\n",
    "x = x.to(device=device, dtype=dtype)\n",
    "y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = F.cross_entropy(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
